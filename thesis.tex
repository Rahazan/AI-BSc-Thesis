\documentclass[11pt,leqno,a4paper]{report} %fleqn


\usepackage{natbib}
\usepackage{hyperref}

\bibliographystyle{abbrvnat}
\usepackage{booktabs}

\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Radboud University Nijmegen}\\[1.5cm] % Name of your university/college
\textsc{\Large Bachelor Thesis}\\[0.5cm] % Major heading such as course name
\textsc{\large Artificial Intelligence}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Classifying cognitive load from galvanic skin response time domain features}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{By:}\\
Guido \textsc{Zuidhof} % Your name
\\\small{guido.zuidhof@student.ru.nl}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
Louis \textsc{Vuurpijl}\\ 
Pashiera \textsc{Barkhuysen}\\
Ervin \textsc{Poljac}
\end{flushright}
\end{minipage}\\[4cm]

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\tableofcontents 
\addcontentsline{toc}{chapter}{Introduction}

\begin{abstract}
To do.
\end{abstract}


\chapter{Introduction}

To do: Short introductory story

\section{Cognitive load}
\emph{Cognitive load theory} \citep{Sweller1994} revolves around the idea that (short-term) working memory is limited, whereas long-term memory is unlimited. 

It is built upon \emph{schema theory}, in which expertise in an area is a function of the acquisition of particular schemata \citep{bartlett1995remembering}. A schema is a mental structure to organise knowledge. \citep{mcvee2005schema} provides a review of schema theory.

When processing information, we use working memory. A schema is one element in this working memory, no matter how large it is \citep{mayer2014cambridge}. 

Cognitive load theory is nearly always studied in the context of learning new schemata. The idea is that there are ways to reduce cognitive load by utilising long-term memory, and thus having more space in working memory, which would allow for more efficient learning.

Cognitive load theory distinguishes three types of cognitive load, which according to research can each be reduced in specific ways \citep{mayer2002multimedia}:


\paragraph{Extraneous cognitive load}
This cognitive load is influenced by how the material or task is presented. It can be reduced by using worked-out examples, using diagrams, or presenting the material in multiple modalities \citep{mousavi1995reducing}.

\paragraph{Intrinsic cognitive load}
This cognitive load is determined by the complexity of the material at hand. It can be reduced by splitting up the task or building upon former (informal) knowledge.

\paragraph{Germane cognitive load}
Germane cognitive load is the cognitive load associated with the process of building, acquiring and automating new schemas. The greater the space in working memory for this load (by reducing the intrinsic and extraneous cognitive load), the greater the ease of learning.

\subsection{Applications}
There are many applications where knowing the cognitive load of a person yields valuable information. Here I will highlight two:

\paragraph{Learning}
Most applications and research regarding cognitive load theory is applied in the context of multimedia learning \citep{brunken2003direct}. The goal in this domain is to find the way to structure and present information to ease learning the most. In other words, to reduce the intrinsic and extraneous cognitive load as much as possible, so that more working memory is left for germane cognitive load.


\paragraph{Usability testing}
Cognitive load measures, and more generally stress and arousal measures, are used by human computer interaction (HCI) developers and researchers to evaluate the usability of software or systems \citep{Jacob2003} \citep{Schmutz2009}.

\citep{Schmutz2009} investigated cognitive load measurement in web applications. Concluded was that cognitive load measures can be used as a \textit{"valuable additional measure of efficiency"}, which strongly correlated to general satisfaction of the user. 

\section{Determining cognitive load}
To do: How this cognitive load could be determined, i.e. by physiological measurement.
\citep{brunken2003direct} is a good survey

\section{Physiological response to cognitive load}
To do: How a human body responds to cognitive load physiologically, why this measurement method/approach is a good idea. 

\subsection{Galvanic Skin Response}

Galvanic Skin Response (GSR) is an interesting physiological response to measure as sensors are cheap and simple, and the subject is not or marginally restricted in movement. To this end GSR is the biometric of choice in this explorative study.

For this reason mobile GSR measurement systems, for instance around the foot \citep{Gravenhorst} are being developed to measure (patients) throughout the day. Also, new devices such as smartwatches are fitted with GSR electrodes increasingly often, which means findings could be applied there. An example would be using GSR data measured by a smart watch to dynamically change an application's presentation of information.

GSR can be measured in two ways, by measuring conductance (in siemens) or resistance (in ohms). Measuring skin conductance \textit{"bears a simpler more linear relationship to the underlying processes of psychological interest than its repriprocal, SR [skin resistance]"} \citep{lykken1971direct}, therefore any GSR measurement in this thesis is done by measuring skin conductance.

\section{Research aim}
The goal of this thesis is to investigate the possibility of determining what the cognitive load of a person is, given GSR measurements. An explorative experiment will be used with different tasks, each of which evokes a different level of cognitive load. From this data a classifier is built, which can classify which task the user was doing based on features derived from GSR, and thus say something about the cognitive load of a person.

The research question reads



\section{State of the art}
Galvanic skin response readings have been studied in regard to cognitive load.

\citep{Shi2007} investigated the possibility of using GSR as an index of cognitive load during the use of unimodal and multimodal versions of the same interface. It has been shown that multimodal interfaces \textit{"support the user in managing cognitive load"} \citep{Oviatt2004}, and thus multimodal interfaces have a lower corresponding cognitive load. 

Shi et al. found that GSR values increase when cognitive load levels increase across all subjects, and conclude that GSR can be used to "serve as an objective indicator of [a] user's cognitive load".

\citep{Nourbakhsh2012} showed that both time and frequency domain features from GSR data significantly correlate with the cognitive load of a person.

In the follow-up research GSR and blink features were used for cognitive load classification \citep{Nourbakhsh2013}. Combining these two featuresets, around 75\% accuracy for binary classification, and 50\% accuracy for four-class classification was achieved. Only GSR features lead to accuracies of around 35\% and 66\% for binary and 4-class classification resepctively.

\section{Organization of this thesis}


\chapter{Method}

\section{Experiment}
The experiment is mostly based on the arithmetic task experiment conducted by Nourbakhsh et al. \citep{Nourbakhsh2012}. The experiment involves adding up four numbers, displayed one by one, and selecting an answer from three possible answers.

There are four difficulty levels of this task. Difficulty one involves adding up four binary numbers (1 and 0). Difficulty level two, three and four consists of adding up  four numbers with a length of one, two and three digits respectively.

The subject is first shown an amount of stars corresponding to the difficulty level for 8 seconds, after which the numbers are shown one by one for 4 seconds. Three answers are then presented on the screen, of which one is correct. The subject then clicks the correct answer using a standard computer mouse. There was no time limit for selecting an answer.

In \citep{Nourbakhsh2013} these tasks were shown to have significant difference in subjective ratings of task difficulty levels, as well as a siginificant difference in response time of different task levels. From which is shown that these designed tasks manipulate cognitive load effectively.


\subsection{Apparatus}
The task was displayed in a 15.6" laptop screen, input was given using a standard computer mouse.
Two galvanic skin response sensors were used in this experiment. The participants wore both at the same time on the left arm. Below, the two sensors are briefly discussed.


\paragraph{Affectiva Q Sensor}
This sensor is embedded in a wrist band, it uses a dry electrode. The sensor side of the wrist was placed on the bottom of the wrist. The sampling rate of this sensor was set to 8 samples per second.

\paragraph{BIOPAC MP30}
The BIOPAC MP30 was used in conjunction with finger GSR sensors. This sensor wraps around two fingers, in this experiment the index and ring finger. It uses gel electrodes, Grass EC33 Electrode Paste was used for this purpose.

The sampling rate was set to 1000 samples per second.

\subsection{Participants}
Six participants took part in this experiment, five male and one female. Their age ranged between 19 and 25 at the time of the experiment. They were not compensated for their participation.


\subsection{Task}

\subsection{Experimental Design}
The task consisted of 8 arithmetic tasks, with 4 difficulty levels. Three of every difficulty level were completed by the participants, in a random order. 

A single task consisted of adding up four numbers, and selecting an answer answer from three options. The numbers to add were shown one by one, for four seconds. Before a task starts, a number of stars is shown equal to the difficulty level, for eight seconds. The tasks followed eachother without breaks. There was no time limit for selecting the answer, there was no feedback whether the selected answer was correct.


\subsection{Procedure}
The participants were first explained the task and seated in front of the laptop. Then the sensors were applied to the left hand and wrist. The participant was then instructed to place the left hand on the table in front of them and not to move it, in order to prevent possible motion artifacts which have been shown to influence GSR readings \citep{motionart}.

\section{Analysis}

\subsection{Affectiva Q Sensor Data}
The data gathered from the Affectiva Q Sensor proved unusable. In most participants no proper contact seemed to have been made with the skin during the full experiment. Dry GSR sensors need some sweat to ensure a good connection with both electrodes. Given the short experiment duration (a few of minutes) this connection was likely never made.

In hindsight the participants could have worn the device longer before the first task start, or they could have been asked to perform some light exercise. 

Any data discussed from here on is solely that collected using the BIOPAC MP30 and it's finger electrodes.


\subsection{Preprocessing}
All data files were preprocessed using custom scripts written in \emph{MATLAB (The MathWorks, Inc.)}. The collected data contained a DC current, which was smoothed by filtering and resampling to 100 samples per second using a Savitzky-Golay filter \citep{savitzky1964smoothing}. 

The data per subject was then cut into seperate frames, the tasks. A task (slice of data) runs from the time that the first number is shown, to the time that an answer is given. These tasks were then smoothed one last time, to get rid of a small remaining amount of DC noise. This was again done using a Savitzky-Golay filter.

The data was then normalized per subject over all datapoints in all tasks. This was done by dividing every data point of the subject by the average of the subject over all tasks. 

Let the average over all tasks:


\[
AverageGSR(s) = \frac 
{\sum_{q \in Q_s}\sum_{t \in T_q} GSR(s,q,t)}
{ \sum_{q \in Q_s} \left\vert{T_q}\right\vert }
\]

With $s$ a given subject, $Q_s$ a set of all tasks performed by $s$, and $T_q$ the set of all measurements in task $q$, and $GSR(s,q,t)$ be the GSR datapoint of subject s, task q at time t.

The normalised GSR is then given by:

\[
NormGSR(s,q,t) = \frac{GSR(s,q,t)}{AverageGSR(s)}
\]

Note that the normalization is done by dividing by the average of the subject over all tasks, and not by simply transforming all measurements to be between 0 and 1. What (original) GSR value would become 1 and 0 would then be quite arbitrary per subject, and not all subjects would be on the same scale, as outliers can totally influence this range.

This normalised GSR was then cut into the specific tasks.

\subsection{Transformation}
From the measurements from every task certain time domain features were extracted. 

The extracted features:

\begin{itemize}

\item Average over all points in the task.
\item Accumulative of all points in the task.
\item Standard deviation of all points in the task.
\item Difference between first and last point of the task.
\item Peaks with varying tresholds of what a peak is.
\end{itemize}

Below I will elaborate how these features were calculated from the data. In these formula's $s$ is a subject, $q$ is a task and $T_q$ are all the measurements (datapoints) associated with task $q$.

\paragraph{Average}

\[
AvgTaskGSR(s,q) = \frac { \sum_{t \in T_q} NormGSR(s,q,t) }
{\left\vert{T_q}\right\vert }
\]

\paragraph{Accumulative}

\[
AccTaskGSR(s,q) =  \sum_{t \in T_q} NormGSR(s,q,t)
\]

\paragraph{Standard deviation}

\[
StdDevTaskGSR(s,q) = \sqrt {\frac{1}{\left\vert{T_q}\right\vert}   \sum_{t \in T_q} (NormGSR(s,q,t) - AvgTaskGSR(s,q))^2 }
\]

\paragraph{Difference}

\[
DiffTaskGSR(s,q) = NormGSR(s,q, t_{\left\vert{T_q}\right\vert}) - NormGSR(s,q, t_1)
\]
where $t_1$ is the first measurement in $T_q$, and $t_{\left\vert{T_q}\right\vert}$ the last measurement of the task.

\paragraph{Peaks}
For peak detection the \emph{PeakFinder} script \citep{yoder}, found in the \emph{Matlab Central File Exchange} was used under BSD License. This script finds local maxima or minima, even in noisy signals, and can be supplied with a treshold that determines what classifies as a peak.

This script was called with four different threshold levels, given by

\[
Threshold(s,q,k) = \frac 
{\max_{t \in T_q} NormGSR(s,q,t) - \min_{t \in T_q} NormGSR(s,q,t)} 
{(2k)^2}
\]

with $k \in \{1,2,3,4\}$.


\subsection{Data mining}

These extracted features are then fed into \emph{Weka 3} \citep{weka}, which is data mining software in Java. 


Three of the most commonly used classification algorithms were used \citep{small}. Namely a Bayesian classification algorithm (\emph{Na\"iveBayes}, which is an implementation based on \citep{john1995estimating}), a support vector machine (SVM) classification algorithm (\emph{LibSVM}) \citep{libsvm} and a neural network classifier (\emph{MultilayerPerceptron}, with 8 hidden layers). 

Two classifiers  were trained with each algorithm, one for 4-class classification, and one for 2-class (binary) classification (with task difficulty 1 and 2 combined, and 3 and 4 combined).


\chapter{Results}
\section{Classification accuracy}
\begin{table}[h]
\caption {Classication accuracy} 
\center
\begin{tabular}{@{}lrr@{}}
\toprule
Classification Algorithm & \multicolumn{1}{l}{2-Class Classification} & \multicolumn{1}{l}{4-Class Classification} \\ \midrule
NaiveBayes               & 62.5\%                                     & 35.4\%                                     \\
LibSVM                   & 68.8\%                                     & 22.9\%                                     \\
MultilayerPerceptron     & 56.3\%                                     & 41.7\%                                     \\ \bottomrule
\end{tabular}
\end{table}

The performance of each predictive model was estimated using 10 fold cross-validation. The percentages in the table are the percentage of correctly classified task difficulties by the predictive model.
\pagebreak

\section{Confusion matrices}
% Confusion matrix NaiveBayes

\begin{table}[h]
\caption {NaiveBayes confusion matrix} 
\center
\begin{tabular}{ccccl}
\multicolumn{1}{l}{1} & \multicolumn{1}{l}{2} & \multicolumn{1}{l}{3} & \multicolumn{1}{l}{4} & \small{\textless- Classified as difficulty} \\ \cline{1-4}
1                          & 7                          & 1                          & \multicolumn{1}{c|}{3}     & Difficulty 1                    \\
3                          & 7                          & 0                          & \multicolumn{1}{c|}{2}     & Difficulty 2                    \\
2                          & 4                          & 2                          & \multicolumn{1}{c|}{4}     & Difficulty 3                    \\
2                          & 2                          & 1                          & \multicolumn{1}{c|}{7}     & Difficulty 4                   
\end{tabular}
\end{table}



% Confusion matrix LibSVM

\begin{table}[h]
\caption {LibSVM confusion matrix} 
\center
\begin{tabular}{ccccl}
\multicolumn{1}{l}{1} & \multicolumn{1}{l}{2} & \multicolumn{1}{l}{3} & \multicolumn{1}{l}{4} & \small{\textless- Classified as difficulty} \\ \cline{1-4}
1                          & 5                          & 6                          & \multicolumn{1}{c|}{0}     & Difficulty 1                    \\
5                          & 3                          & 3                          & \multicolumn{1}{c|}{1}     & Difficulty 2                    \\
3                          & 1                          & 5                          & \multicolumn{1}{c|}{3}     & Difficulty 3                    \\
5                          & 0                          & 5                          & \multicolumn{1}{c|}{2}     & Difficulty 4                   
\end{tabular}
\end{table}

% Confusion matrix MultilayerPerceptron

\begin{table}[h]
\caption {MultilayerPerceptron confusion matrix} 
\center
\begin{tabular}{ccccl}
\multicolumn{1}{l}{1} & \multicolumn{1}{l}{2} & \multicolumn{1}{l}{3} & \multicolumn{1}{l}{4} & \small{\textless- Classified as difficulty} \\ \cline{1-4}
3                          & 4                          & 3                          & \multicolumn{1}{c|}{2}     & Difficulty 1                    \\
3                          & 5                          & 2                          & \multicolumn{1}{c|}{2}     & Difficulty 2                    \\
2                          & 3                          & 6                          & \multicolumn{1}{c|}{1}     & Difficulty 3                    \\
2                          & 1                          & 3                          & \multicolumn{1}{c|}{6}     & Difficulty 4                   
\end{tabular}
\end{table}







\chapter{Discussion}
\section{Interpretation}
The results show that the classification accuracy of all three classifiers are better than chance dictates, however not much. In the binary classification task the results were more reasonable.
These classification rates are slightly lower than those reported by \citep{Nourbakhsh2013} in a similar experiment, this may be due to the smaller dataset.

When considering the confusion matrices of the 4-class classification it can be noted that most wrongly classified task difficulties was task 1, which was more often classified as difficulty 2 and 3 than 1. In fact, it has only been classified correct once out of twelve entries in the NaiveBayes and LibSVM model, and a mere three times in the MultilayerPerceptron model. Perhaps this task was too easy and caused participants to become distracted.


There are many possible explanations why this classification accuracy is not greater than it is. The first explanation is that from just GSR data, it simply can not be predicted any better. Next I will highlight some other possible reasons:

\subsubsection{Loss of information from filter}
Filtering the data, to remove the DC current noise, lead to some slight loss of data. It smoothed peaks slightly, which may have influenced the classification rate.


\subsubsection{Task bleeding}
Another explanation may be the bleeding of tasks into other tasks. If for instance a subject first gets two hard tasks, and then an easy one, it is imaginable that the subject´s GSR value for that easy task is higher than it would have been without first having done harder tasks. 

This is mitigated by the random order in which the tasks are presented and the time between tasks, however with the relatively small subject pool it is not unimaginable this has had it´s effect on the classification accuracy.


\subsubsection{Dataset size}
With 6 subjects and 8 tasks per subject, the total dataset consisted of 48 entries. Training a classification algorithm on a small dataset may cause outliers to influence it to fairly great effect.

\subsubsection{Task cognitive load manipulation}
Despite \citep{Nourbakhsh2013} showing that the tasks effectively manipulate the cognitive load of the subject, this was concluded from subjective reports and response times. It may be that these response times were manipulated by something other than cognitive load, and that the tasks do not evoke a difference in cognitive load in the subject. It may also be that this difference in cognitive load is very small, making classification harder, especially with more than two classes.

\subsubsection{Frequency domain features}
All extracted features were features from the time domain, frequency domain features were shown by \citep{Nourbakhsh2012} to significantly correlate with the task difficulty as well. Adding frequency domain features to the featureset, may improve classification rates.

\section{Future research}
Future research could be measuring GSR data using longer, carefully designed tasks, of which the cognitive load can be determined using an established cognitive load index.
Also future research could include investigate how classification accuracy changes when using data measured by more portable (consumer) sensors, whose measurements are likely of lesser quality than the used lab equipment in this experiment.
 




% -- Bibliography
\bibliography{ref}{}
%\bibliographystyle{plain}

\end{document}
