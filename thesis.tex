\documentclass[12pt,leqno,letterpaper]{report} %fleqn
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{0.18cm}


%\usepackage{cite}
\usepackage{natbib}
\usepackage{hyperref}
%\bibliographystyle{plainnat}
\bibliographystyle{abbrvnat}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Radboud University Nijmegen}\\[1.5cm] % Name of your university/college
\textsc{\Large Bachelor Thesis}\\[0.5cm] % Major heading such as course name
\textsc{\large Artificial Intelligence}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Classifying cognitive load from galvanic skin response time domain features}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Guido \textsc{Zuidhof} % Your name
\\\small{guido.zuidhof@student.ru.nl}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
Louis \textsc{Vuurpijl} % Supervisor's Name
Pashiera \textsc{Barkhuysen}
Ervin \textsc{Poljac}
\end{flushright}
\end{minipage}\\[4cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\begin{abstract}
\end{abstract}

\tableofcontents 
\addcontentsline{toc}{chapter}{Introduction}



\chapter{Introduction}
\section{Cognitive load}
\emph{Cognitive load theory} \citep{Sweller1994} revolves around the idea that (short-term) working memory is limited, whereas long-term memory is unlimited. 

It is built upon \emph{schema theory}, in which expertise in an area is a function of the acquisition of particular schemata \citep{bartlett1995remembering}. A schema is a mental structure to organise knowledge. \citep{mcvee2005schema} provides a review of schema theory.

When processing information, we use working memory. A schema is one element in this working memory, no matter how large it is \citep{mayer2014cambridge}. 

Cognitive load theory is nearly always studied in the context of learning new schemata. The idea is that there are ways to reduce cognitive load by utilising long-term memory, and thus having more space in working memory, which would allow for more efficient learning.

Cognitive load theory distinguishes three types of cognitive load, which according to research can each be reduced in specific ways \citep{mayer2002multimedia}:


\paragraph{Extraneous cognitive load}
This cognitive load is influenced by how the material or task is presented. It can be reduced by using worked-out examples, using diagrams, or presenting the material in multiple modalities \citep{mousavi1995reducing}.

\paragraph{Intrinsic cognitive load}
This cognitive load is determined by the complexity of the material at hand. It can be reduced by splitting up the task or building upon former (informal) knowledge.

\paragraph{Germane cognitive load}
Germane cognitive load is the cognitive load associated with the process of building, acquiring and automating new schemas. The greater the space in working memory for this load (by reducing the intrinsic and extraneous cognitive load), the greater the ease of learning.


\section{Determining cognitive load}
\citep{brunken2003direct}
How this cognitive load could be determined, i.e. by measurement.

\section{Physiological response to cognitive load}
How a human body responds to cognitive load/stress. 


\subsection{Measurement of physiological response}
Measuring heart rate, breathing, eeg, gsr, etc.

With an increasing variety in consumer 

\paragraph{Galvanic Skin Response}


%\subsubsection{Classification}
%How sense can be made from this data

\subsection{Applications}
How this can be used.

\paragraph{Learning}
Most applications and research regarding cognitive 


\paragraph{Usability testing}
Cognitive load measures, and more generally stress and arousal measures, are used by human computer interaction (HCI) developers and researchers to evaluate the usability of software or systems \citep{Jacob2003} \citep{Schmutz2009}.

Schmutz et al. \citep{Schmutz2009} investigated cognitive load measurement in web applications. Concluded was that cognitive load measures can be used as a \"valuable additional measure of efficiency", which strongly correlated to general satisfaction of the user. 



\section{Research aim}
Using features extracted from GSR data to determine cognitive load, data gathered from experiment.

\section{State of the art}
Galvanic skin response readings have been studied in regard to cognitive load.

\citep{Shi2007} investigated the possibility of using GSR as an index of cognitive load during the use of unimodal and multimodal versions of the same interface. It has been shown that multimodal interfaces "support the user in managing cognitive load" \citep{Oviatt2004}, and thus multimodal interfaces have a lower corresponding cognitive load. 

Shi et al. found that GSR values increase when cognitive load levels increase across all subjects, and conclude that GSR can be used to "serve as an objective indicator of [a] user's cognitive load".

\citep{Nourbakhsh2012} showed that 

In the follow-up research by Nourbakhsh et al. \citep{Nourbakhsh2013} GSR and blink features were used for cognitive load classification. Combining these two featuresets, around 75\% accuracy for binary classification, and 50\% accuracy for four-class classification was achieved. Only GSR features lead to accuracies of around 35\% and 66\% for binary and 4-class classification resepctively.



\chapter{Method}

\section{Experiment}
The experiment is mostly based on the arithmetic task experiment conducted by Nourbakhsh et al. \citep{Nourbakhsh2012}. The experiment involves adding up four numbers, displayed one by one, and selecting the right from three possible answers.

There are four difficulty levels of this task. Difficulty one involves adding up binary numbers (1 and 0). Difficulty level two, three and four consists of numbers with a length of one, two and three digits respectively.

The subject is first shown an amount of stars corresponding to the difficulty level for 8 seconds, after which the numbers are shown one by one for 4 seconds. Three answers are then presented on the screen, of which one is correct. The subject then clicks the correct answer using a standard computer mouse. There was no time limit for selecting an answer.

In \citep{Nourbakhsh2013} these tasks were shown to have significant difference in subjective ratings of task difficulty levels, as well as a siginificant difference in response time of different task levels. From which is shown that these designed tasks manipulate cognitive load effectively.


\subsection{Apparatus}
The task was displayed in a 15.6" laptop screen, input was given using a standard computer mouse.
Two galvanic skin response sensors were used in this experiment. The participants wore both at the same time on the left arm.

\subsection{Participants}
Six participants took part in this experiment, five male and one female. Their ages ranged between 20 and 25 at the time of the experiment. They were not compensated for their participation.


\subsection{Affectiva Q Sensor}
This sensor is embedded in a wrist band, it uses a dry electrode. The sensor side of the wrist was placed on the bottom of the wrist. The sampling rate of this sensor was set to 8 samples per second.

\subsection{BIOPAC MP30}
The BIOPAC MP30 was used in conjunction with finger GSR sensors. This sensor wraps around two fingers, in this experiment the index and ring finger. It uses gel electrodes, Grass EC33 Electrode Paste was used.

The sampling rate was set to 1000 samples per second.

\subsection{Task}

\subsection{Experimental Design}
The task consisted of 12 arithmetic tasks, with 4 difficulty levels. Three of every difficulty level were completed by the participants, in a random order. 

The task consisted of adding up three numbers, and selecting the right answer from three options. The numbers to add were showed one by one, for four seconds. Before a task starts, a number of stars is shown equal to the difficulty level, for eight seconds. The tasks followed eachother without breaks. There was no time limit for selecting the answer, there was no feedback whether the selected answer was correct.


\subsection{Procedure}
The participants were first explained the task and seated in front of the laptop. Then the sensors were applied to the left hand and wrist. The participant is then instructed to place the left hand on the table in front of them and not to move it, in order to prevent movement or motion artifacts.

\section{Analysis}

\subsection{Affectiva Q Sensor Data}
The data gathered from the Affectiva Q Sensor proved unusable. In most participants no proper contact seemed to have been made with the skin during the full experiment. Dry GSR sensors need some sweat to ensure a good connection with both electrodes. Given the short experiment duration (a few of minutes) this connection was likely never made.

In hindsight the participants could have worn the device longer before the first task start, or they could have been asked to perform some light exercise. 

Any data discussed from here on is solely that collected using the BIOPAC MP30 and it's finger electrodes.


\subsection{Preprocessing}
The collected data contained a DC current, which was smoothed by filtering and resampling to 100 samples per second using Savitzky-Golay filtering. 
The data per subject was then cut into seperate frames, the tasks. 

The data was then normalized per subject over all datapoints in all tasks. This was done by dividing every data point of the subject by the average of the subject over all tasks. 

Let the average over all tasks:


\[
AverageGSR(s) = \frac 
{\sum_{q \in Q_s}\sum_{t \in T_q} GSR(s,q,t)}
{ \sum_{q \in Q_s} \left\vert{T_q}\right\vert }
\]

With $s$ a given subject, $Q_s$ a set of all tasks performed by $s$, and $T_q$ the set of all measurements in task $q$, and $GSR(s,q,t)$ be the GSR datapoint of subject s, task q at time t.

The normalised GSR is then given by:

\[
NormGSR(s,q,t) = \frac{GSR(s,q,t)}{AverageGSR(s)}
\]

Note that the normalization is done by dividing by the average of the subject over all tasks, and not by simply transforming all measurements to be between 0 and 1. What (original) GSR value would become 1 and 0 would then be quite arbitrary per subject, and not all subjects would be on the same scale, as outliers can totally influence this range.

This normalised GSR 


These tasks were then smoothed one last time, to get rid of a small remaining amount of DC noise. This was again done using a Savitzky-Golay filter.

\subsection{Transformation}
From the measurements from every task certain time domain features were extracted. 

The extracted features:

\begin{itemize}

\item Average over all points in the task.
\item Accumulative of all points in the task.
\item Standard deviation of all points in the task.
\item Difference between first and last point of the task.
\item Peaks with varying tresholds of what a peak is.
\end{itemize}

Below I will elaborate how these features were calculated from the data. In these formula's $s$ is a subject, $q$ is a task and $T_q$ are all the measurements (datapoints) associated with task $q$.

\paragraph{Average}

\[
AvgTaskGSR(s,q) = \frac { \sum_{t \in T_q} NormGSR(s,q,t) }
{\left\vert{T_q}\right\vert }
\]

\paragraph{Accumulative}

\[
AccTaskGSR(s,q) =  \sum_{t \in T_q} NormGSR(s,q,t)
\]

\paragraph{Standard deviation}

\[
StdDevTaskGSR(s,q) = \sqrt {\frac{1}{\left\vert{T_q}\right\vert}   \sum_{t \in T_q} (NormGSR(s,q,t) - AvgTaskGSR(s,q))^2 }
\]

\paragraph{Difference}

\[
DiffTaskGSR(s,q) = NormGSR(s,q, t_{\left\vert{T_q}\right\vert}) - NormGSR(s,q, t_0)
\]
where $t_0$ is the first measurement in $T_q$, and $t_{\left\vert{T_q}\right\vert}$ the last measurement of the task.

\paragraph{Peaks}
For peak detection the \emph{PeakFinder} script \citep{yoder}, found in the \emph{Matlab Central File Exchange} was used under BSD License. This script finds local maxima or minima, even in noisy signals, and can be supplied with a treshold that determines what classifies as a peak.

This script was called with four different threshold levels, given by

\[
Threshold(s,q,k) = \frac 
{\max_{t \in T_q} NormGSR(s,q,t) - \min_{t \in T_q} NormGSR(s,q,t)} 
{(2k)^2}
\]

with $k \in \{1,2,3,4\}$.


\subsection{Data mining}


\chapter{Results}

\section{..?}
\section{..?}

\chapter{Discussion}
\section{Interpretation}
The results show that from the results


\subsubsection{Loss of information from filter}
Filtering the data, to remove the DC current noise, lead to some slight loss of data. It smoothed peaks ever so slightly, this may have influenced the classification rate.


\subsubsection{Task bleeding}
Another explanation may be the bleeding of tasks into other tasks. If for instance a subject first gets two hard tasks, and then an easy one, it is imaginable that the subject´s GSR value for that easy task is higher than it would have been without first having done harder tasks. 

This is mitigated by the random order in which the tasks are presented and the time between tasks, however with the relatively small subject pool it is not unimaginable this has had it´s effect on the classification accuracy.

\subsubsection{Dataset size}
With 6 subjects and 8 tasks per subject, the total dataset had 48 entries. Training a classification algorithm on a small dataset may cause outliers to influence it to fairly great effect.

\subsubsection{Task cognitive load manipulation}
Despite \citep{Nourbakhsh2013} showing that the tasks effectively manipulate the cognitive load of the subject, this was concluded from subjective reports and response times. It may be that these response times were manipulated by something other than cognitive load, and that the tasks do not evoke a difference in cognitive load in the subject. It may also be that this difference in cognitive load is very small, making classification harder, especially with more than two classes.

\subsubsection{Frequency domain features}
All extracted features were features from the time domain, frequency domain features were shown by \citep{Nourbakhsh2012} to significantly correlate with the task difficulty as well. Adding frequency domain features to the featureset, may improve classification rates.

\subsection{Future research}
Future research could include investigating how viable this method is given more portable (consumer) sensors in a real life setting. 





% -- Bibliography
\bibliography{ref}{}
%\bibliographystyle{plain}

\end{document}
